<!-- TOC -->

    - [ZBS设计优点](#zbs设计优点)
- [专有名词](#专有名词)
- [SMTX 块存储服务技术白皮书](#smtx-块存储服务技术白皮书)
  - [基本概念](#基本概念)
    - [**传统数据中⼼架构**](#传统数据中架构)
    - [**软件定义的存储**](#软件定义的存储)
    - [**超融合架构**](#超融合架构)
  - [ZBS简介](#zbs简介)
  - [架构与核心组件](#架构与核心组件)
  - [存储数据结构](#存储数据结构)
    - [**块(Block)**—256KB](#块block256kb)
    - [**数据块(extend)**—256MB](#数据块extend256mb)
    - [**条带化**（striping）—便利⽤多个磁盘的 IO 能⼒](#条带化striping便利多个磁盘的-io-能)
    - [**存储卷（Volume）**—up to 64TB](#存储卷volumeup-to-64tb)
    - [**数据存储**—存储卷的集合](#数据存储存储卷的集合)
  - [数据管理](#数据管理)
    - [**存储池**](#存储池)
    - [**拓扑感知**](#拓扑感知)
    - [**精简配置**](#精简配置)
    - [**访问权限**](#访问权限)
    - [**接⼊点管理**](#接点管理)
  - [数据IO路径](#数据io路径)
    - [**元数据设计**](#元数据设计)
    - [**全局 IO 路径**](#全局-io-路径)
      - [**iSCSI IO 路径**](#iscsi-io-路径)
      - [**NFS IO 路径（网络文件系统）**](#nfs-io-路径网络文件系统)
    - [**本地 IO 路径**](#本地-io-路径)
      - [**本地磁盘分区**](#本地磁盘分区)
      - [**Journal**](#journal)
      - [**ExtentStore**](#extentstore)
    - [**副本分配策略**——管理面重点内容](#副本分配策略管理面重点内容)
    - [**副本迁移策略**——管理面重点内容](#副本迁移策略管理面重点内容)
      - [**数据访问局部化**](#数据访问局部化)
      - [**拓扑再均衡**](#拓扑再均衡)
      - [**容量均衡**](#容量均衡)
  - [数据保护](#数据保护)
    - [1. **副本**（客户端同时写n（1-3）副本后返回）](#1-副本客户端同时写n1-3副本后返回)
    - [2. **数据校验**](#2-数据校验)
    - [3. **快照**](#3-快照)
    - [4. **双活集群**](#4-双活集群)
    - [5. **异步复制**](#5-异步复制)
  - [数据接入模式](#数据接入模式)
  - [异常处理](#异常处理)
  - [与计算平台的集成](#与计算平台的集成)

<!-- /TOC -->

## ZBS设计优点

1. 副本分配策略时尽量本地化（包括虚机迁移后仍然会再次进行本地化），来提升性能；
2. io可配条带化下发，提升性能；
3. 数据副本数可配；
4. 本地优先、 拓扑安全 、 局部化分配、负载均衡、访问模式感知、数据动态移动、副本分配策略动态调整等策略的互相平衡，是ZBS设计的一大难点和优点。

# 专有名词

柜顶式交换机（Top of Rack, TOR）

机架组（pod）、机 架（rack）、机箱（brick）、节点（node）

# SMTX 块存储服务技术白皮书

## 基本概念

### **传统数据中⼼架构**

数据中⼼在过去的⼏⼗年间发⽣了巨⼤变化，先后经历了⼤型机、独⽴服务器、集中存储、虚拟化技术 的出现和成熟等主要阶段。这些不断演进的技术，相当好地解决了各个时代数据中⼼所⾯临的问题，然 ⽽当随着时代的进步和科技的发展，这些传统的技术，尤其是存储⽅⾯的技术，在应对爆炸式增⻓的海 量数据时，越来越显⽰出不⾜之处。如今⼤量使⽤的仍然是传统数据中⼼架构：⼀般由负责计算的服务 器加外部的存储阵列。存储设备之间通过存储区域⽹络（Storage Area Network - SAN）进⾏数据传输和 通信。这样的架构在应对海量数据时，有如下⼏个主要缺点：

**1.** 计算和存储分离。计算资源位于服务器，当计算资源需要访问数据时，必须通过存储⽹络进⾏，⼤⼤增加了延迟。

**2.** 传统的存储设备上都有存储控制器，所有的数据读写都要经它处理，⽽它的读写吞吐能⼒远远低于计算服务器处理器的读写吞吐能⼒。当计算服务器的处理器对存储设备提出读写请求时，尤其是⼤量的读写请求时，由于存储控制器的瓶颈，使得整个系统响应读写的速度⼤⼤降低。 

**3.** 同时也因为上述原因 2，计算服务器的处理器需要⻓时间等待读写操作完成后才能进⾏下⼀步操作，导致其利⽤率⼤⼤降低。 

**4.** ⼤量使⽤专属存储硬件，且控制这些专属硬件的软件与硬件⾼度耦合。因为是专属硬件，没有统⼀的业界标准，不同⼚商的设备很有可能互不兼容。遇到不得不更新设备以适应新的业务需求时，投⼊相当巨⼤； 

**5.** 各种专属硬件存储设备都有⾃⼰的管理机制和界⾯，使得管理变得复杂； 

**6.** 不适合进⾏横向扩展。横向扩展（亦称线性扩展），简⾔之就是⽤更多的节点⽀撑更⼤量的请求。由于传统存储设备上的存储控制器能控制的 CPU、内存和存储插槽有限，所以存储空间的扩展也⼤⼤受限。

由此可⻅，传统的存储⽅式成为数据中⼼性能得以提⾼的瓶颈。为了解决上述的缺点，出现了软件定义的存储和超融合架构的概念。  



### **软件定义的存储**

软件定义的存储的核⼼是存储系统的管理和智能层⾯与底层硬件分离，将存储硬件中关键的控制、处理功能（如存储控制器、去重、压缩等）抽出来由软件实现。这种⽅式使得存储服务可以运⾏在通⽤的存储硬件上，不再需要专属硬件。这就意味着，存储控制软件不仅可以控制甲⼚商的存储设备，也可以控制⼄丙丁等其他⼚商的存储设备。由于数据的可⽤性由软件提供，⽽不是冗余的硬件设备提供，这⼤⼤降低了投⼊成本，原因在于：

（1）昂贵的专属硬件被便宜得多的通⽤硬件代替；

（2）许多原来由硬件 实现的功能现在由软件实现，省去了购买这部分硬件的成本。

<u>存储服务通常包括快照、去重、多副本、克隆、压缩、精简配置等</u>，这些功能在传统存储中由硬件实现；⽽软件定义的存储，则由软件提供这些服务，从⽽不再跟专属硬件⾼度耦合，⽽且可以通过简单的软件升级轻松提升已有功能和添加新的功能。因此，存储的灵活性和适应性⼤⼤提⾼。

<u>软件定义存储以虚拟技术和池化存储资源为基础</u>。分布在同⼀集群的不同服务器上的存储资源能够⽅便地组成⼀个逻辑上的存储池供⽤⼾使⽤和管理，仿佛使⽤⼀块⼤的本地存储盘。

### **超融合架构**

<u>简单地说，超融合架构是节点间没有明确的计算和存储的分⼯，在同⼀个服务器硬件资源（⽬前主要 是 x86 服务器）上实现核⼼的存储和计算功能，封装为单⼀的、⾼度虚拟化的解决⽅案。之所以能把计算和存储资源集成到同⼀台服务器中，其基础是近些年处理器技术和存储设备硬件本⾝性能的⼤幅度提⾼。</u> 

超融合架构的核⼼特征和优势：

**1.** 计算、存储和⽹络等资源融合在同⼀台服务器上，所以数据的读取可以优先在本地存储进⾏，只有当本地存储没有相应数据时，才会从远程节点读取，有效地减少跨⽹络读取数据的开销。 

**2.** 软件定义的存储。 

**3.** 读写性能的⼤幅度提⾼，使得计算服务器的处理器的使⽤率也得到⼤幅度提升。 

**4.** 不再使⽤或很少使⽤专属硬件，⼤⼤降低投⼊成本。 

**5.** ⼀个统⼀的管理⼊⼝，管理各种资源。

**6.** ⽅便进⾏横向扩展。横向扩展时，是通过增加新的节点来满⾜对存储的需求。⼀般来说，⼀个节点就是⼀台 x86 服务器。这种扩展的最⼤优点在于，在扩⼤存储容量的同时，由于 x86 服务器还含有 CPU、内存以及存储插槽等资源，所以扩展后的集群，性能也同时得到了提升。值得注意的是，这种性能的提升，可以根据实际需要实现精细化⽽变得更灵活：例如当希望计算能⼒得到⼤幅度提升时，可以偏重增加计算资源；当希望存储能⼒得到⼤幅度提升时，可以偏重增加存储资源。得益于以通⽤硬件为基础的软件定义架构，在集群部署完成后，可以通过节点硬件资源的动态热插拔实现节点⻆⾊/集群能⼒的弹性调整以应对不断变化的业务需求。前⾯讲过，传统存储的扩展受制于其存储控制器能⼒的上限，⽽超融合结构由于使⽤了软件定义的存储，其存储控制器由软件实现，每个节点上都可以⽅便地运⾏存储控制器，这种机制使得集群的节点数的扩展不再受制于存储控制器。横向扩展可以根据实际业务需求，从⼩规模扩展开始，因此投⼊也较低，⽤⼾也不必提前预估数年后对存储的需求。

对⽐下传统的存储架构，可以发现，超融合架构基本避免或者⼤⼤缓解了传统存储架构存在的问题。



## ZBS简介

## 架构与核心组件

## 存储数据结构

### **块(Block)**—256KB

块（block） 是 ChunkServer 内部进⾏数据存储的基本单元。⼀个数据块（extent）由多个块组成。块同时也是缓存盘与数据盘之间进⾏数据交换的基本单位。块默认⼤⼩为 256 KiB

### **数据块(extend)**—256MB

数据块（extent）是 MetaCluster 管理的基本单元。⼀个存储卷被切分成多个固定⼤⼩的数据块，存储在ChunkServer 中。在 ZBS 的集群中，数据恢复、迁移、写时复制（CopyOnWrite- COW）等，都是以数据块为单位进⾏的。数据块默认⼤⼩为 256 MiB。

### **条带化**（striping）—便利⽤多个磁盘的 IO 能⼒

条带化（striping）是把连续的数据分割成相同⼤⼩的条带（stripe），尽量把每条数据分别写⼊到不同磁盘上的⽅法，以便利⽤多个磁盘的 IO 能⼒。ZBS 中条带数可以设置为 1、2、 4（默 认值），<u>设置的条带数越⼤，并⾏越⾼，对于顺序 IO 的性能更好。每个条带⼤⼩可以设置为 4、8、16、32、64、128、256 KiB （默认值）。设置太⼤容易造成并发度不够，对性能没有帮助，太⼩容易造成顺序 IO 变成随机 IO，破坏集群整体的性能。</u>

### **存储卷（Volume）**—up to 64TB

存储卷（volume），是 ZBS 对外提供的最基本的数据结构。它可以对应到⼀个虚拟机的存储卷，也可以对应到 iSCSI 中的⼀个 LUN。

<u>存储卷由多个固定⼤⼩数据块组成，**数据块表（extent table） ⽤于记录某个存储卷所包含的数据块 ID**。当客⼾端需要在存储卷上执⾏ IO 操作时，会通过数据块表，查找到对应的数据块 ID，以及数据块所在的 ChunkServer 副本位置信息。</u>

由于存储卷由多个数据块组成，且这些数据块分布在不同的节点上，因此存储卷的容量可以超过单个物理服务器的总存储空间。存储卷也⽀持精简配置（thin provisioning），最⼤可以⽀持创建 64 TiB 的存储卷。

除此之外，<u>存储卷还具有很多属性，⽐如可以为其配置不同的存储策略，包括副本数、条带化参数等等。存储卷也是⽤⼾执⾏快照、克隆、回滚等操作的基本单位</u>。

### **数据存储**—存储卷的集合

数据存储（datastore）是⼀组存储卷的集合。<u>数据存储中包含的存储卷都具有相同的存储策略，例如副本数、精简配置等。每⼀个数据存储，都属于某⼀个特定的存储池。</u>这个数据存储中创建的所有存储卷中包含的数据，也都存储在这个存储池中。



## 数据管理

### **存储池**

<u>存储池是 ZBS 对存储介质进⾏组织的单元。在 ZBS 存储服务集群中，不同的存储介质可以加⼊不同的存储池，使得存储池具备不同的存储特性</u>，例如：SSD/HDD 混合存储池，全 SSD 存储池等（<u>在⽬前 ZBS 的实现中，服务器是可以加⼊存储池的最⼩单元</u>）。

<u>存储池提供了数据物理隔离的能⼒，⼀份数据的所有副本，都会保存在同⼀个存储池中</u>。数据在存储池之间并不共享。如果数据需要在存储池之间进⾏移动，需要触发数据拷⻉。



### **拓扑感知**

ZBS 的数据分配是拓扑敏感的。尽可能地将数据 分散⾄物理拓扑的不同分⽀上，降低任⼀设备（存储节点/⽹络设备/电源模块）故障对系统的影响。

<u>为了利用拓扑感知带来的可靠性提升，配置存储池时，应该尽量选取位于不同拓扑分⽀的节点组成存储池</u>



### **精简配置** 

通常的使⽤场景⾥，虚拟机申请的磁盘空间并不是⽴即⽤满的。例如⼀个虚拟机申请了⼀个 1 T 的磁盘作为数据存储，并不是在申请的当时就⽴刻填充满 1T 的磁盘空间，⽽是在虚拟机上承载的业务运⾏过程中逐渐的填充数据：即对数据空间的需求通常是随着时间逐渐提出的。<u>为了提⾼存储空间的有效利⽤率，节省成本，ZBS 采⽤了精简配置策略。</u>**<u>以数据块为粒度**，仅在触发真实使⽤数据空间时（数据写⼊）才在 ChunkServer 中分配对应的空间。</u>

精简配置默认开启，<u>在开启了精简配置之后， ZBS 提供的名义逻辑存储空间⼤⼩可以超过实际物理空间⼤⼩</u>。可以参考⽇常的业务空间需求来调节空间超分的⽐例，<u>在实际空间需求达到真实可⽤空间限制之前补充设备扩展空间即可。</u>



### **访问权限**   

为了提供数据访问安全等级，ZBS 为 iSCSI ⽬标/NFS Export 提供⽩名单机制。可以为每个 NFS Export/ iSCSI ⽬标指定允许访问该对象的 IP地址列表。接⼊层在处理接⼊请求时将校验客⼾端的 IP 地址。对于 不符合⽩名单中匹配规则的来源 IP，将拒绝该请求。

对 iSCSI 数据存储以及 target 下的每⼀个 LUN，都可以单独设定⽩名单，⽤以隔离访问客⼾端，防⽌被 ⾮认证客⼾端访问篡改数据。

<u>ZBS iSCSI 服务⽀持使⽤ CHAP 进⾏访问权限控制，⽀持单向认证和双向认证。单向认证指 iSCSI Target 可对 Initiator 端提供的⽤⼾名和密码进⾏⾝份认证。双向认证在单向认证的基础上，Initiator 端对 iSCSI Target 端也要进⾏⾝份认证。</u>



### **接⼊点管理**   

所有挂载给外部使⽤的 iSCSI Target 上的数据链路都由 Meta 中的 iSCSI Service 统⼀管理。按照如下策略为每条数据链路分配接⼊点： 

 ·同⼀个 Target 被不同的 Initiator 连接时尽量使⽤不同的接⼊点（在 Target 作为共享数据被多个 Initiator 的情况下这个策略可以保证集群接⼊点性能被最⼤化的利⽤）；

 · 同⼀个 Initiator 连接不同 Target 时尽量使⽤不同的接⼊点（在单个 Initiator 连接多个对象时候，可以保证 Initiator 可以使⽤整个集群的资源）；

 · 每个 Access 上维持的活跃链路数量基本⼀致。



## 数据IO路径

### **元数据设计** 

在分布式存储系统中，元数据服务通常都是设计的难点。如果设计不当，⾮常容易成为系统的瓶颈。

ZBS 元数据管理具有以下⼏个特点：

**1.** <u>采⽤集中式元数据管理，通过收集集群中数据的状态，做到对集群中数据的精确控制，包括数据的分配、数据的恢复和迁移等等。</u>   

**2.** 采⽤较⼤粒度的基本数据单元（数据块），减少元数据消耗的内存资源，保证全部元数据都可以保存在内存中，提⾼访问效率。   

**3.** 元数据在所有节点中都保存⼀份副本，保证元数据的⾼可靠。当有部分服务器发⽣宕机，元数据不会丢失。   

**4.** 通过对元数据进⾏缓存，减少各个组件与 MetaServer 的交互，从⽽减⼩ MetaServer 的负载，提⾼ IO 的效率。

### **全局 IO 路径**

<u>ZBS 采⽤了 iSCSI 和 NFS 两种存储协议</u>

#### **iSCSI IO 路径**

如果数据在缓存中存在，则缓存系统可以直接响应请求。否则，请求将被转发给持久化层。

第10~14，IO 请求执⾏完成，返回给 AccessServer

<u>对于写请求，只有所有副本都写成功才会返回成功</u>。当因为⽹络异常或存储设备异常⽽导致部分副本写⼊失败时，AccessServer 会⽴即把问题副本从集群中剔除，并触发数据恢复流程；如果⽹络正常，且写操作正在进⾏，但 30 秒内仍未完成写⼊，则会终⽌写操作，由 AccessServer 把问题副本从集群中剔除，并触发数据恢复流程。如果所有副本都写失败，则会⽴即触发重试操作。<u>对于读请求，如果有⼀个副本读取失败，则会⽴即尝试从其他副本读取。</u>

#### **NFS IO 路径（网络文件系统）**

<u>NFS 协议并不直接⽀持 IO 重定向</u>，所以 NFS 的 IO 路径在寻址上与 iSCSI 稍有不同。为保证 NFS 服务的⾼可⽤性，需要在 NFS 客⼾端所在的物理节点上部署 IO Rerouter 程序。NFS 客⼾端通过固定的服务地址访问 NFS 服务（例如：192.168.33.2）。IO Rerouter 程序定期检查当前连接的 AccessServer 的存活状态，在 AccessServer 异常时， IO Rerouter 将负责修改 NFS 客⼾端所在的服务器的路由表，把 192.168.33.2 指向⼀个当前可⽤的 AccessServer，NFS 客⼾端即可切换⾄新的可⽤ AccessServer 继续IO。

### **本地 IO 路径**

#### **本地磁盘分区**

OS(45GB)、MetaData(20GB)、Journal(10GB)、Cache — 以上数据保存在SSD中，通过raid1实现备份。

#### **Journal**

<u>Journal ⽤于记录 ChunkServer 本地的数据操作，包括数据块的创建、删除、修改等等。每个写⼊请求都会写⼊⼀条 Journal</u>，当写⼊请求⼤⼩⼩于 block ⼤⼩时，Journal 中包含完整的请求数据,。⽽当缓存命中（即缓存有⾜够的空间供相关数据写⼊或相关数据已经在缓存中存在）且请求⼤⼩为 block ⼤ ⼩时（常⻅于顺序写场景，接⼊层会将顺序 IO 合并⾄ block ⼤⼩下发⾄ Chunk），数据会直接写⼊缓存，并在稍后异步同步到 ExtentStore。此时 Journal 不携带数据，仅记录操作元数据，避免了写放⼤。

每⼀个 ChunkServer 中会包含多个 Journal 。当其中⼀个 Journal 写满时，会切换到另⼀个 Journal 。 当写满的 Journal 中的数据全部同步到 ExtentStore 中以后，写满的 Journal 会被清空，可以被重新使⽤。<u>Journal 位于 SSD 上，以提供⾜够⾼的 IO 能⼒。</u>  

**CacheManager**

<u>CacheManager 通过近期最少使⽤算法（Least Recently Used, LRU），缓存⽤⼾频繁访问的数据</u>。当数据第⼀次被写时，数据会直接写到 ExtentStore 上（如果是随机 IO，则会先写到 Journal 中）。当数据再次被访问时，会被缓存到 CacheManager 中进⾏管理。

#### **ExtentStore** 

ExtentStore ⽀持对数据进⾏校验。如果开启数据校验，则每⼀个保存的数据块都会同时保存⼀个 Checksum 值。当数据被访问时，会通过 Checksum 来校验存储数据的正确性。



### **副本分配策略**——管理面重点内容

为了保证 IO 的⾼效，MetaServer 会通过收集集群信息，对副本分配做出近似最优的选择。相关影响因素包括:

**1.** 本地优先。由于 ZBS ⾯向超融合虚拟化场景设计，虚拟化的 Hypervisor 与 ChunkServer 会运⾏在同 ⼀台物理主机上。所以 MetaServer 会尽量将副本中的其中⼀个，分配在与 Hypervisor 运⾏在相同的物理主机上的 ChunkServer 中，以减少存储服务对⽹络的压⼒；

**2.** 拓扑安全。数据副本尽可能分布在物理拓扑的不同分⽀上，减少物理故障带来的损害； 

**3.** 局部化。单个卷的数据分布的范围应当较为集中，⽽不是分散在整个集群中。不同的卷尽可能使⽤不同的数据空间，降低卷对物理资源占⽤的冲突可能，实现集群整体性能的线性扩展能⼒并降低节点故障的影响范围； 

**4.** 负载均衡。尽可能地让所有存储节点的实际承担容量近似，以便平均访问负载并降低节点异常时的损失； 

**5.** 访问模式感知。通过多种⽅式感知到卷的访问模式。例如是否多点访问（多个虚拟机使⽤同⼀个虚拟磁盘）、是否作为模板等。针对不同的访问模式进⾏副本位置调整；

 **6.** <u>数据动态移动。当数据访问需求⽅在集群中移动时（例如超融合模式下虚拟机迁移），对应数据副本需要随之移动，以达到数据本地化；</u>

**7.** <u>副本分配策略动态调整。副本分配的多个⽬标之间并不是协调⼀致的，例如本地优先会导致数据集中，⽽负载均衡希望不同节点数据分散。</u>所以需要根据当前集群的节点空间使⽤⽐例，调整不同⽬标的优先级，使⽤不同的策略；

其中 1、2、3、4 为数据副本分配的基本⽬标。其中拓扑安全⽬前设计为⾼优先级，但⾮强制性⽬标。即在拓扑安全可满⾜的条件下拓扑安全将作为第⼀优先级。若⽆法满⾜，则允许暂时按照不安全的⽅式进⾏数据分配，稍后在可能的情况下再⾃动修复。⽽本地优先与局部化分配没有⽭盾，可以同时处理，但它们在某些场景下（虚拟机需要的数据空间集中在少数节点上）与容量均衡的⽬标是互相冲突的。<u>ZBS 将根据不同的负载情况，动态调整本地优先，局部化分配，容量均衡三个策略之间的优先级。</u>

按照集群中数据空间已分配⽐例最⾼节点的磁盘空间已分配⽐例 p 作为标准，策略⽬标优先级调整如 下：

**1.** 低负载状态（p < 75%）：此时节点间的容量均衡被忽略，以本地优先与数据局部化为⽬标分配与调整数据，优先级次序为 ： ② 本地优先 = ① 拓扑安全 > ③ 局部化分配； 

**2.** 中负载状态（75% <= p < 85%）：此时新副本分配还是按照本地优先与数据局部化为⽬标，优先级次序与低负载时相同。但是迁移扫描将根据当前 IO 状态，尝试迁移部分⾮本地且⾮活跃数据以达到容量均衡。再平衡时对⾮本地活跃数据的优先级次序为： ②本地优先 = ① 拓扑安全 > ④ 容量均衡 > ③ 局部化分配 ； 

**3.** ⾼负载状态（85% <= p）：分配时的数据局部化⽬标将暂时被忽略，以本地优先 + 容量均衡为⽬标进⾏数据分配。数据调整时，除中负载时的⾮本地且⾮活跃数据之外，还会尝试迁移包含本地的⾮活跃数据。此时根据 Extent 的 prefer local 节点本⾝的负载，在初次分配副本时有所不同： 

a. Prefer local 节点⾃⾝的容量没有超过 85%，优先级次序为 ②本地优先 = ① 拓扑安全 > ④ 容量均衡 ；

b. Prefer Local 节点⾃⾝的容量已经超过 85%，放弃本地优先，仅采⽤ ① 拓扑安全 > ④ 容量均衡策略； 

**4.** 不同负载边界时的处理：为避免节点容量始终处于策略判定的容量边界导致频繁反复变更优先顺序⽽ 导致的集群内数据不断被来回迁移，影响正常 IO，会增加容忍阈值，默认 5%。例如当节点容量超过 75% 后不会⽴即触发中等负载的平衡策略，⽽是需要等到节点容量>= 80% 时才触发迁移，尝试将节点容量降低⾄ 75% 以下。

访问模式感知，⽬前 ZBS 感知以下两种访问模式：

**1.** 快照模板化：当⼀个快照被多次克隆（默认超过 10次），则它会被处理为模板，关联的数据将在集 群中整体均匀分布（忽略局部化与本地化⽬标）。在 Elf 平台下，还可以通过将存储卷/虚拟机显式地 指定为模板触发存储卷的均匀分布。 

**2.** 池化访问，当⼀个 Volume 同时被多个接⼊点访问时，Volume 会被认为是⼀个虚拟的存储池，此时 不会根据接⼊点迁移数据；

<u>数据动态移动则是通过接⼊点感知完成，当⼀个活跃的存储卷的访问点发⽣变化（例如虚拟机迁 移），ZBS 接⼊服务将上报给 MetaServer，MetaServer 在后续的再平衡策略中将数据逐步朝访问点迁移以达到数据本地化⽬标。</u>

对于双活集群，其副本分配策略略有不同，具体请参⻅双活集群副本机制（⻅第 21 ⻚）中对副本分配机制的描述。



### **副本迁移策略**——管理面重点内容

<u>⽣产集群中的环境会经常发⽣变化，例如有服务器磁盘故障、服务器故障、集群扩容、虚拟机迁移等等。</u>

这些变化将导致最初的副本分布在新的场景下并不⼀定是最佳的分布。为了保证集群的最佳分布，MetaServer 会定期对集群状态进⾏检查，并触发副本迁移任务。副本迁移主要为了达到以下三个⽬标：

#### **数据访问局部化**

在虚拟化环境中，虚拟机会经常根据需求进⾏迁移。当虚拟机发⽣迁移时，部分原本是本地化的访问，将变成是远程访问。为降低因远程访问带来的额外开销，MetaServer 会触发数据向本地迁移。

通常，为避免不必要的迁移，MetaServer 会在虚拟机迁移以后等待⼀段时间，再触发数据的迁移。在迁移的时候，会优先迁移被频繁访问的数据。如果迁移的⽬标端的空间已经⽤满，则不会触发迁移。

#### **拓扑再均衡**

当集群因扩容或其他原因，导致集群中拓扑发⽣变化，为保证集群健康，MetaSever 将会根据最新的拓扑尝试调整数据的期望分配，以达到最佳效果。

#### **容量均衡**

当集群处在中⾼负载状态时（有节点的存储空间利⽤率 > 75%）为了避免数据过度集中，MetaServer 会定期的将⾼负载节点上的⾮本地热数据迁移⾄其他节点，以降低数据集中访问的⻛险。

除此之外，ZBS 也并不会追求容量绝对的均衡。在集群中，允许⼀定程度的不均衡情况出现。如集群处于低负载状态（所有节点存储利⽤率均低于 75%）则不会触发容量均衡，以避免集群中的数据出现抖动。集群处于⾼负载状态时，会⾃动加快迁移检查频率，尽快达到均衡状态。





## 数据保护

### 1. **副本**（客户端同时写n（1-3）副本后返回）

### 2. **数据校验**

除了副本技术之外，为了提供更⾼级别数据安全保证，ZBS 同时对所存储的数据进⾏了数据块校验。每 ⼀份存储在 ZBS 中的数据，都会同时保存⼀份该数据的校验码。ZBS 在数据被访问时，对数据进⾏校 验，以避免磁盘硬件⽆记录数据损坏（ Silent Data Corruption - SDC） 的发⽣。当发⽣数据与数据校验 码不⼀致时，则认为该数据块存在问题，同时会触发数据恢复机制。

### 3. **快照**

ZBS 的快照采⽤<u>元数据快照和写时复制（CopyOnWrite - COW） 技术</u>，可以实现秒级快照。

副本与数据校验可以有效地避免由于硬件故障导致数据安全问题。⽽快照，则可以⽤于避免因⼈为误操 作或软件故障所导致的数据损坏或丢失。

### 4. **双活集群** 

双活集群是指⼀个含有两个可⽤域的集群。⼀个可⽤域通常指⼀个数据中⼼，这个数据中⼼可以完全独 ⽴地提供计算和存储服务。属于同⼀个双活集群的两个可⽤域，除⾮特殊说明，⼀般是位于同⼀个城 市。当两个域都⼯作正常时，集群会确保每个数据块在两个可⽤域内都保存有副本，当⼀个可⽤域失效 时，可以从存活的可⽤域中获得完整的数据恢复业务。

 * **双活集群副本机制**:

   双活集群的保护能⼒，源于其多副本机制。<u>双活集群中副本数量的分配基本原则是：优先可⽤域保留 2 个副本，次级可⽤域保留 1 个副本；</u>副本迁移和分配的原则基本同⾮双活集群，但⼜有些差别。

### 5. **异步复制**

ZBS 提供本地快照计划和异地异步复制功能，其中异步复制⽬前仅⽀持异地⽬标站点也是 ZBS 集群的情形。

异步复制实际上就是将快照数据搬运到⼀个或多个异地存放的过程。但传输数据的⽹络通常需要经过⼴域⽹，带宽⾮常有限。为此，ZBS 对异步复制功能内建了许多数据传输量的优化，⼤⼤加快了异步复制数据传输的速度。

与双活集群的实时复制不同，异步复制是以复制⾮实时的快照到异地为基础，所以对延时要求低，从⽽使得受保护站点和异步复制⽬标站点的距离，在带宽⾜够的情况下，可以达数千公⾥。

## 数据接入模式

## 异常处理

## 与计算平台的集成





* Storage Area Network - SAN：

  存储设备之间通过存储区域⽹络（Storage Area Network - SAN）进⾏数据传输和通信。

  